/* quantize_v2_test */
/*
 * A test which shows how to run quantize op when quantization parameters
 * are inputted from MLU device.
 *
 * define tensors: input and output
 * input size: 64 x 3 x 224 x 224
 * input data type: float32
 * output size: 64 x 3 x 224 x 224
 * input data type: int16
 */
#include <bang.h>
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#include <fstream>
#include <iostream>
#include <sstream>
#include <string>

#include "cnnl.h"
#include "cnrt.h"

void fun(cnnlTensorDescriptor_t desc_input) {}
int main() {
  // get current device 0
  unsigned int device_cnt;
  cnrtDev_t dev;                  // 声明 设备变量
  cnrtQueue_t queue = nullptr;    // 声明 队列
  cnnlHandle_t handle = nullptr;  // 声明 句柄变量
  /* init device 初始化设备 */
  std::cout << "Begin:init device:\n";
  CNRT_CHECK(cnrtInit(
      0));  // 初始化运行环境, 0 表示初始化实际设备, 1 表示初始化虚拟设备
  CNRT_CHECK(
      cnrtGetDeviceCount(&device_cnt));  // 获取设备个数 必须在 cnrtInit(0)
                                         // 之后才能正确获取设备个数
  std::cout << "device count:" << device_cnt << std::endl;
  if (device_cnt <= 0) {
    std::cout << "There is no MLU device.\n";
    return 0;
  }

  CNRT_CHECK(cnrtGetDeviceHandle(&dev, 0));  // 获取设备 0 的句柄,存储在 dev 中
  CNRT_CHECK(cnrtSetCurrentDevice(
      dev));  // 设置当前线程 使用的设备句柄 (据此设置该线程使用的设备)

  CNRT_CHECK(cnrtCreateQueue(&queue));  // 定义一个 队列变量 pQueue

  cnnlCreate(&handle);  // 创建 cnnl 句柄，此句柄会默认与
                        // cnrtSetCurrentDevice(dev) 中的设备(设备0)绑定
  cnnlSetQueue(handle,
               queue);  // 将队列 pQueue 绑定到 handle 中. 用于调用
                        // cnnl 函数时，确定需要用到的 队列 queue
  std::cout << "End:init device:\n";

  // prepare input and output
  const int dimension_num = 4;
  const int ni = 1, ci = 3, hi = 3, wi = 2;
  const int no = 1, co = 3, ho = 3, wo = 2;
  size_t input_count = ni * ci * hi * wi;
  size_t output_count = no * co * ho * wo;
  size_t size_input = input_count * sizeof(float);
  size_t size_output = output_count * sizeof(int16_t);
  cnnlDataType_t input_data_type = CNNL_DTYPE_FLOAT;
  cnnlDataType_t output_data_type = CNNL_DTYPE_INT16;

  // prepare desc of input
  int input_dimension[dimension_num] = {ni, ci, hi, wi};
  cnnlTensorDescriptor_t desc_input;
  cnnlCreateTensorDescriptor(&desc_input);
  cnnlSetTensorDescriptor(desc_input, CNNL_LAYOUT_NCHW, input_data_type,
                          dimension_num, input_dimension);

  // prepare desc of output
  int output_dimension[dimension_num] = {no, co, ho, wo};
  cnnlTensorDescriptor_t desc_output;
  cnnlCreateTensorDescriptor(&desc_output);
  cnnlSetTensorDescriptor(desc_output, CNNL_LAYOUT_NCHW, output_data_type,
                          dimension_num, output_dimension);

  // prepare cpu buffer and origin data
  float *input_cpu_data = (float *)malloc(input_count * sizeof(float));
  int16_t *output_cpu_data = (int16_t *)malloc(output_count * sizeof(int16_t));
  unsigned int seed = time(0);
  float input_data[] = {5, 1, 8.0, 1.0, 6, 4, 3, 8, 2,
                        6, 0, 6,   8,   5, 7, 4, 9, 6};
  for (int i = 0; i < input_count; ++i) {
    // input_cpu_data[i] = ((rand_r(&seed) % 100 / 100.0) - 0.5) / 2;
    input_cpu_data[i] = input_data[i];
  }

  for (int i = 0; i < 10; i++) {
    std::cout << input_cpu_data[i] << ",";
  }
  std::cout << "\n";

  // prepare position, scale, and quantize mode
  // the position and scale just for a sample demonstration here
  cnnlQuantizeMode_t mode = CNNL_QUANTIZE_POSITION_SCALE;
  int position = -9;
  float scale = 1.123f;

  // malloc mlu buffer
  void *input_mlu_ptr = nullptr;
  void *output_mlu_ptr = nullptr;
  void *position_mlu_ptr = nullptr;
  void *scale_mlu_ptr = nullptr;
  CNRT_CHECK(cnrtMalloc((void **)&input_mlu_ptr, size_input));
  CNRT_CHECK(cnrtMalloc((void **)&output_mlu_ptr, size_output));
  CNRT_CHECK(cnrtMalloc((void **)&position_mlu_ptr, sizeof(int32_t)));
  CNRT_CHECK(cnrtMalloc((void **)&scale_mlu_ptr, sizeof(float)));

  // copy input data to mlu device
  CNRT_CHECK(cnrtMemcpy(input_mlu_ptr, input_cpu_data, size_input,
                        CNRT_MEM_TRANS_DIR_HOST2DEV));
  // copy quantization parameters to mlu device
  CNRT_CHECK(cnrtMemcpy(position_mlu_ptr, &position, sizeof(int32_t),
                        CNRT_MEM_TRANS_DIR_HOST2DEV));
  CNRT_CHECK(cnrtMemcpy(scale_mlu_ptr, &scale, sizeof(float),
                        CNRT_MEM_TRANS_DIR_HOST2DEV));

  // 尝试计算 参数

  size_t workspace_size = 0;
  void *workspace = nullptr;

  // 获取 workspace size

  cnnlGetQuantizeParamWorkspaceSize(handle, desc_input, &workspace_size);

  if (workspace_size != 0) {
    CNRT_CHECK(cnrtMalloc((void **)(&workspace), workspace_size));
    CNRT_CHECK(cnrtMemset(workspace, 0, workspace_size));
  }
  std::cout << workspace_size << "\n";

  cnnlQuantizeParam(handle, cnnlQuantizeMode_t::CNNL_QUANTIZE_POSITION_SCALE,
                    desc_input, input_mlu_ptr, 16, workspace, workspace_size,
                    position_mlu_ptr, scale_mlu_ptr, nullptr);

  CNRT_CHECK(cnrtSyncQueue(queue));
  CNRT_CHECK(cnrtMemcpy(&position, position_mlu_ptr, sizeof(int32_t),
                        CNRT_MEM_TRANS_DIR_DEV2HOST));
  CNRT_CHECK(cnrtMemcpy(&scale, scale_mlu_ptr, sizeof(float),
                        CNRT_MEM_TRANS_DIR_DEV2HOST));
  cnrtSyncQueue(queue);

  std::cout << position << " " << scale << "\n";
  ///////////////////

  // cnnlQuantizeV2 compute
  cnnlQuantizeV2(handle, mode, desc_input, input_mlu_ptr, position_mlu_ptr,
                 scale_mlu_ptr, nullptr, desc_output, output_mlu_ptr);

  cnrtSyncQueue(queue);
  cnnlSetTensorDescriptorPositionAndScale(desc_output, position, scale);

  // wait for computing task over
  cnrtSyncQueue(queue);

  cnnlQuantizeV2(handle, mode, desc_input, input_mlu_ptr, position_mlu_ptr,
                 scale_mlu_ptr, nullptr, desc_output, output_mlu_ptr);

  fun(desc_output);

  // copy output data to cpu
  CNRT_CHECK(cnrtMemcpy(output_cpu_data, output_mlu_ptr, size_output,
                        CNRT_MEM_TRANS_DIR_DEV2HOST));
  for (int i = 0; i < 10; i++) {
    std::cout << output_cpu_data[i] << ",";
  }
  std::cout << "\n";
  // free mlu buffer
  if (input_mlu_ptr) {
    CNRT_CHECK(cnrtFree(input_mlu_ptr));
  }
  if (output_mlu_ptr) {
    CNRT_CHECK(cnrtFree(output_mlu_ptr));
  }
  if (position_mlu_ptr) {
    CNRT_CHECK(cnrtFree(position_mlu_ptr));
  }
  if (scale_mlu_ptr) {
    CNRT_CHECK(cnrtFree(scale_mlu_ptr));
  }

  // free cpu buffer
  free(input_cpu_data);
  free(output_cpu_data);

  // free input and ouput descriptor
  cnnlDestroyTensorDescriptor(desc_input);
  cnnlDestroyTensorDescriptor(desc_output);

  // free handle
  cnnlDestroy(handle);

  // free quene
  // cnrtQueueDestroy(queue);
  CNRT_CHECK(cnrtDestroyQueue(queue));

  return 0;
}
