/**
 * @file main.cpp
 * @author liuzengqiang
 * @brief multiply add sample impplemented by BANG on Cambrian MLU, ref:samples/cnnl/convolution.sample
 *
 * Input(NHWC)(1*3*3*2): [[[[5, 1], [8, 1], [6, 4]],
 *                       [[3, 8], [2,6], [0, 6]],
 *                       [[8, 5], [7,4], [9, 6]]]]
 *
 * Bias(1*1*1*1): [[[[1]]]]
 *
 * Filter(1*3*3*2): [[[[1, 5], [7,5], [4, 2]],
 *                  [[1, 8], [3,8], [6, 2]],
 *                  [[4, 8], [5,0], [9, 5]]]]
 * Pad: [1,1,1,1]
 *
 * Stride: [2,2]
 *
 * Dilation: [1,1]
 *
 * Expected Outputs(1*2*2*1): [[[[136], [122]],
 *                            [[195], [176]]]]
 * @version 0.1
 * @date 2023-06-26
 *
 * @copyright Copyright (c) 2023
 *
 */

#include "cnnl.h"
#include "cnrt.h"
#include <bang.h>
#include <iostream>
#include <stdio.h>
#include <stdlib.h>
#include <string>
#include <time.h>

struct KernelAttributes
{
    int h = 0;
    int w = 0;
    int stride_h = 1;
    int stride_w = 1;
    int dilation_h = 1;
    int dilation_w = 1;
};

struct DataShape
{
    int n = 0;
    int c = 0;
    int h = 0;
    int w = 0;
    inline int size()
    {
        return n * c * h * w;
    }
};

struct PadAtrributes
{
    int pad_top = 0;
    int pad_bottom = 0;
    int pad_left = 0;
    int pad_right = 0;
};
/**
 * @brief 数据的 shape 参数, 与 conv_sample.cc 不同, 本文将 ShapeParam 与 DataTypeParam 分离
 *
 */
struct ShapeParam
{
    DataShape input;
    DataShape weight; // filter
    DataShape output;
    DataShape bias;
    KernelAttributes kernel;
    PadAtrributes pad;
    bool has_bias = false;
    int group_count = 1;
};

/**
 * @brief 数据的 type 参数
 *
 */
struct DataTypeParam
{
    cnnlDataType_t input_dtype;
    cnnlDataType_t weight_dtype;
    cnnlDataType_t output_dtype;
    // !!! 注意 这里数据的格式是 NHWC 不是 NCHW !!!
    cnnlTensorLayout_t layout = CNNL_LAYOUT_NHWC;
};

/**
 * @brief Data address structure
 *
 */
struct DataAddress
{
    // 如果不使用 定点量化技术， 就不需要 cpu_data, fp_data，只需要 host 和 device
    float *cpu_data = nullptr; // repack quantized data used for cpu compute
    float *fp_data = nullptr;  // store init random data

    void *host_data = nullptr;   // store casted data
    void *device_data = nullptr; // mlu device address
    size_t mlu_size = 0;         // mlu malloc size
    size_t fp_size = 0;          // cpu random data size
};
/**
 * @brief Convolution data structure
 *
 */
struct Convolution
{
    /* 数据描述符 */
    cnnlTensorDescriptor_t input_desc = nullptr;     // 输入数据 描述符
    cnnlTensorDescriptor_t weight_desc = nullptr;    // weight 数据描述符
    cnnlTensorDescriptor_t output_desc = nullptr;    // 输出数据 描述符
    cnnlTensorDescriptor_t bias_desc = nullptr;      // bias 数据描述符
    cnnlConvolutionDescriptor_t conv_desc = nullptr; // convolution 卷积描述符
    cnnlConvolutionForwardAlgo_t algo;               //  卷积算法

    /* 数据地址, 数据大小 */
    DataAddress input_data;
    DataAddress weight_data;
    DataAddress output_data;
    DataAddress bias_data;

    /* 辅助workspace空间 */
    void *workspace = nullptr; // 辅助工作空间 workspace
    size_t workspace_size = 0; // 辅助工作空间 workspace 大小

    double diff1 = 0.0;
    double diff2 = 0.0;
};
/**
 * @brief 初始化 tensor 描述符
 *
 * @param &desc 描述符
 * @param shape data shape
 * @param dtype data type
 * @param layout layout
 */
void setTensorDesc(cnnlTensorDescriptor_t &desc, Shape2D shape, cnnlDataType_t dtype, cnnlTensorLayout_t layout)
{
    int dim[4];
    if (layout == CNNL_LAYOUT_NHWC)
    {
        dim[0] = shape.n;
        dim[1] = shape.h;
        dim[2] = shape.w;
        dim[3] = shape.c;
    }
    else if (layout == CNNL_LAYOUT_NCHW)
    {
        dim[0] = shape.n;
        dim[1] = shape.c;
        dim[2] = shape.h;
        dim[3] = shape.w;
    }
    else if (layout == CNNL_LAYOUT_HWCN)
    {
        dim[0] = shape.h;
        dim[1] = shape.w;
        dim[2] = shape.c;
        dim[3] = shape.n;
    }
    else
    {
        std::cerr << "unsupport data layout!";
    }
    CNNL_CHECK(cnnlCreateTensorDescriptor(&desc));
    CNNL_CHECK(cnnlSetTensorDescriptor(desc, layout, dtype, 4, dim));
}

void initDescriptorAndWorkspace(cnnlHandle_t handle, Convolution &convolution, ShapeParam &shape_param,
                                DataTypeParam &data_type_param)
{
    /* 初始化数据描述符 input, weight, output, bias*/
    setTensorDesc(convolution.input_desc, shape_param.input, data_type_param.input_dtype, data_type_param.layout);
    setTensorDesc(convolution.weight_desc, shape_param.weight, data_type_param.weight_dtype, data_type_param.layout);
    setTensorDesc(convolution.output_desc, shape_param.output, data_type_param.output_dtype, data_type_param.layout);
    // 如果存在 bias 则 初始化 bias 的描述符
    if (shape_param.has_bias)
    {
        setTensorDesc(convolution.bias_desc, shape_param.bias, data_type_param.output_dtype, data_type_param.layout);
    }
    // 卷积描述符的初始化比较复杂
    // 获取 stride, dilation 和 pad 的 attribute
    int stride[2] = {shape_param.kernel.stride_h, shape_param.kernel.stride_w};
    int dilation[2] = {shape_param.kernel.dilation_h, shape_param.kernel.dilation_w};
    int pad[4] = {shape_param.pad.pad_top, shape_param.pad.pad_bottom, shape_param.pad.pad_left,
                  shape_param.pad.pad_right}; // top, bottom, left, right

    CNRT_CHECK(cnnlCreateConvolutionDescriptor(&convolution.conv_desc));
    convolution.algo = CNNL_CONVOLUTION_FWD_ALGO_DIRECT;
    cnrt_check(cnnlSetConvolutionDescriptor(convolution.conv_desc, 4, pad, stride, dilation, shape_param.group_count,
                                            data_type_param.output_dtype));

    // 设置 workspace 辅助空间大小
    CNRT_CHECK(cnnlGetConvolutionForwardWorkspaceSize(
        handle, convolution.input_desc, convolution.weight_desc, convolution.output_desc, convolution.bias_desc,
        convolution.conv_desc, convolution.algo, &(convolution.workspace_size)));

    // 设置 数据内存(host/device端)大小 (byte)
    auto getDataSize = [](cnnlDataType_t dtype) -> size_t {
        switch (dtype)
        {
        case CNNL_DTYPE_HALF:
            return 2;
        case CNNL_DTYPE_FLOAT:
            return 4;
        case CNNL_DTYPE_INT8:
            return 1;
        case CNNL_DTYPE_INT16:
            return 2;
        case CNNL_DTYPE_INT31:
            return 4;
        case CNNL_DTYPE_INT32:
            return 4;
        default:
            std::cout << "unsupport data  dtype.\n";
        }
    };
    convolution.input_data.fp_size = shape_param.input.size() * sizeof(float);
    convolution.input_data.mlu_size = shape_param.input.size() * getDataSize(data_type_param.input_dtype);

    convolution.weight_data.fp_size = shape_param.weight.size() * sizeof(float);
    convolution.weight_data.mlu_size = shape_param.weight.size() * getDataSize(data_type_param.weight_dtype);

    convolution.output_data.fp_size = shape_param.output.size() * sizeof(float);
    convolution.output_data.mlu_size = shape_param.output.size() * getDataSize(data_type_param.output_dtype);
    if (shape_param.has_bias)
    {
        convolution.bias_data.fp_size = shape_param.bias.size() * sizeof(float);
        convolution.bias_data.mlu_size = shape_param.bias.size() * getDataSize(data_type_param.output_dtype);
    }
}
/**
 * @brief 初始化 host 内存空间，数据
 * @param convolution
 * @param shape_param
 * @param input_data
 * @param weight_data
 * @param bias_data
 */
void initHostData(Convolution &convolution, ShapeParam &shape_param, float *input_data, float *weight_data,
                  float *bias_data)
{
    // 申请空间
    convolution.input_data.host_data = (float *)malloc(convolution.input_data.fp_size);
    convolution.weight_data.host_data = (float *)malloc(convolution.weight_data.fp_size);
    if (shape_param.has_bias)
    {
        convolution.bias_data.host_data = (float *)malloc(convolution.bias_data.fp_size);
    }
    // 赋值
    for (int i = 0; i < shape_param.input.size(); i++)
    {
        convolution.input_data.host_data[i] = input_data[i];
    }

    for (int i = 0; i < shape_param.weight.size(); i++)
    {
        convolution.weight_data.host_data[i] = weight_data[i];
    }
    if (shape_param.has_bias)
    {
        for (int i = 0; i < shape_param.bias.size(); i++)
        {
            convolution.bias_data.host_data[i] = bias_data[i];
        }
    }
}
/**
 * @brief 初始化device内存空间
 * @param convolution
 */
void deviceMalloc(Convolution &convolution, ShapeParam &shape_param)
{
    // 初始化 device 内存空间 并且 赋值为 0.0
    CNRT_CHECK(cnrtMalloc(&(convolution.input_data.device), convolution.input_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.input_data.device, 0, convolution.input_data.mlu_size));

    CNRT_CHECK(cnrtMalloc(&(convolution.weight_data.device), convolution.weight_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.weight_data.device, 0, convolution.weight_data.mlu_size));

    CNRT_CHECK(cnrtMalloc(&(convolution.output_data.device), convolution.output_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.output_data.device, 0, convolution.output_data.mlu_size));

    if (shape_param.has_bias)
    {
        CNRT_CHECK(cnrtMalloc(&(convolution.bias_data.device), convolution.bias_data.mlu_size));
        CNRT_CHECK(cnrtMemset(convolution.bias_data.device, 0, convolution.bias_data.mlu_size));
    }

    if (convolution.workspace_size != 0)
    {
        CNRT_CHECK(cnrtMalloc(&(convolution.workspace), convolution.workspace_size));
        CNRT_CHECK(cnrtMemset(convolution.workspace, 0, convolution.workspace_size));
    }
}
/**
 * @brief 初始化 device 内存空间上的数据
 * @param convolution
 */
void initDeviceData(Convolution &convolution, ShapeParam &shape_param)
{
    CNRT_CHECK(cnrtMemcpy(convolution.input_data.device, convolution.input_data.host, convolution.input_data.mlu_size,
                          CNRT_MEM_TRANS_DIR_HOST2DEV));

    CNRT_CHECK(cnrtMemcpy(convolution.weight_data.device, convolution.weight_data.host,
                          convolution.weight_data.mlu_size, CNRT_MEM_TRANS_DIR_HOST2DEV));

    if (shape_param.has_bias)
    {
        CNRT_CHECK(cnrtMemcpy(convconvolution.bias_data.device, convconvolution.bias_data.host,
                              convconvolution.bias_data.mlu_size, CNRT_MEM_TRANS_DIR_HOST2DEV));
    }
}
/**
 * @brief MLU上执行 卷积操作
 * @param handle
 * @param queue
 * @param conv
 */
void convolutionComputeDevice(cnnlHandle_t &handle, cnrtQueue_t &queue, Convolution &conv)
{
    // mlu compute start
    cnrtNotifier_t start = nullptr, end = nullptr;
    cnrtCreateNotifier(&start);
    cnrtCreateNotifier(&end);
    cnrtPlaceNotifier(start, queue);
    // call convolutionForward interface

    CNNL_CHECK(cnnlConvolutionForward(handle, conv.conv_desc, conv.algo, nullptr, conv.input_desc,
                                      conv.input_data.device, conv.weight_desc, conv.weight_data.device, conv.bias_desc,
                                      conv.bias_data.device, conv.workspace, conv.workspace_size, nullptr,
                                      conv.output_desc, conv.output_data.device));

    CNRT_CHECK(cnrtPlaceNotifier(end, queue));
    CNRT_CHECK(cnrtSyncQueue(queue));
    // mlu compute end
    CNRT_CHECK(cnrtNotifierDuration(start, end, &(conv.hardware_time)));

    CNRT_CHECK(cnrtDestroyNotifier(&start));
    CNRT_CHECK(cnrtDestroyNotifier(&end));
}
/**
 * @brief 打印结果
 * @param conv
 */
void printResults(Convolution &conv, ShapeParam &shape_param)
{
    // 把 output_data 传输到 cpu 上
    // 打印数据
}

/**
 * @brief 释放内存
 * @param convolution
 */
void freeDeviceHost(cnnlHandle_t &handle, Convolution &conv, cnrtQueue_t &queue)
{
    // free host space
    free(conv.input_data.host);
    free(conv.input_data.fp_data);
    free(conv.input_data.cpu_data);
    free(conv.weight_data.host);
    free(conv.weight_data.fp_data);
    free(conv.weight_data.cpu_data);
    free(conv.output_data.host);
    free(conv.output_data.fp_data);
    free(conv.output_data.cpu_data);

    if (conv.bias_data.host)
    {
        free(conv.bias_data.host);
        free(conv.bias_data.fp_data);
    }

    // destory op and tensor desc
    CNNL_CHECK(cnnlDestroyConvolutionDescriptor(conv.conv_desc));
    CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.input_desc));
    CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.weight_desc));
    CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.output_desc));
    if (conv.bias_desc)
    {
        CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.bias_desc));
    }

    // free device memory
    CNRT_CHECK(cnrtFree(conv.input_data.device));
    CNRT_CHECK(cnrtFree(conv.weight_data.device));
    CNRT_CHECK(cnrtFree(conv.output_data.device));
    if (conv.bias_data.device != nullptr)
    {
        CNRT_CHECK(cnrtFree(conv.bias_data.device));
    }
    if (conv.workspace != nullptr)
    {
        CNRT_CHECK(cnrtFree(conv.workspace));
    }

    // destory queue and runtime context
    CNRT_CHECK(cnrtDestroyQueue(queue));
    CNNL_CHECK(cnnlDestroy(handle));
}

int main(int, char **)
{

    cnrtRet_t ret;                 // Enumeration variables describing the return values of CNRT APIs.
    cnrtDev_t dev = nullptr;       // 声明 设备变量
    cnnlHandle_t handle = nullptr; // 声明 句柄变量

    unsigned int device_cnt;

    CNRT_CHECK(cnrtInit(i)); // 初始化运行环境, 0 表示初始化实际设备, 1 表示初始化虚拟设备

    CNRT_CHECK(cnrtGetDeviceCount(&device_cnt)); // 获取设备个数 必须在 cnrtInit(0) 之后才能正确获取设备个数
    std::cout << "device count:" << device_cnt << std::endl;
    if (device_cnt <= 0)
    {
        std::cout << "There is no MLU device.\n";
        return -1;
    }

    CNRT_CHECK(cnrtGetDeviceHandle(&dev, 0)); // 获取设备 0 的句柄,存储在 dev 中
    CNRT_CHECK(cnrtSetCurrentDevice(dev)); // 设置当前线程 使用的设备句柄 (据此设置该线程使用的设备)

    CNRT_CHECK(cnnlCreate(&handle)); // 创建 cnnl 句柄，此句柄会默认与 cnrtSetCurrentDevice(dev) 中的设备(设备0)绑定

    cnrtQueue_t pQueue;
    CNRT_CHECK(cnrtCreateQueue(&pQueue)); // 定义一个 队列变量 pQueue

    CNRT_CHECK(cnnlSetQueue(handle,
                            pQueue)); // 将队列 pQueue 绑定到 handle 中. 用于调用 cnnl 函数时，确定需要用到的 队列 queue

    cnnlDataType_t a;
    // 初始化 handle, 数据(NCHW), 生成描述符, 进行计算, 验证数据
    /* 初始化数据 */
    /* 数据格式 NCHW(1,3,10,10): 1个 batch, 3 通道, 数据大小为 10*10 */

    Convolution convolution;

    /* 初始化数据 shape_param */
    ShapeParam shape_param;
    shape_param.input = {1, 3, 3, 2};
    shape_param.weight = {1, 3, 3, 2};
    shape_param.output = {1, 2, 2, 1};
    shape_param.bias = {1, 1, 1, 1};
    shape_param.kernel.h = 3;
    shape_param.kernel.w = 3;
    shape_param.pad = {1, 1, 1, 1};
    shape_param.has_bias = false; // 不启用 bias
    shape_param.group_count = 1;  // 只用一个 group
    /* 初始化 data_type_param */
    DataTypeParam data_type_param;
    data_type_param.input_dtype = cnnlDataType_t::CNNL_DTYPE_FLOAT;
    data_type_param.weight_dtype = cnnlDataType_t::CNNL_DTYPE_FLOAT;
    data_type_param.output_dtype = cnnlDataType_t::CNNL_DTYPE_FLOAT;
    // !!! 注意 这里数据的格式是 NHWC 不是 NCHW !!!
    data_type_param.layout = CNNL_LAYOUT_NHWC;

    /* 初始化 数据描述符 和 workspace 空间 */
    initDescriptorAndWorkspace(handle, convolution, shape_param, data_type_param);

    /* 申请数据空间 */
    deviceMalloc(convolution);
    /* 初始化数据 value */
    // input,
    // weight,
    // bias,
    float input_data[] = {5, 1, 8, 1, 6, 4, 3, 8, 2, 6, 0, 6, 8, 5, 7, 4, 9, 6};
    float weight_data[] = {1, 5, 7, 5, 4, 2, 1, 8, 3, 8, 6, 2, 4, 8, 5, 0, 9, 5};
    float bias[] = {1.0};

    // 先不使用 定点量化 技术

    initHostData(convolution, shape_param, input_data, weight_data, bias);

    initDeviceData(convolution, shape_param);

    cnnlSetQueue();
    cnnlConvolutionForward();

    cnnlSetConvolutionDescriptor();
    cnrtDestroyQueue(pQueue); // 销毁队列
    cnrtDestroy();            // 销毁运行环境, 与 cnrtInit() 匹配
    return 0;
}
