/**
 * @file main.cpp
 * @author liuzengqiang
 * @brief multiply add sample impplemented by BANG on Cambrian MLU, ref:samples/cnnl/convolution.sample
 *
 * Input(NHWC)(1*3*3*2): [[[[5, 1], [8, 1], [6, 4]],
 *                       [[3, 8], [2,6], [0, 6]],
 *                       [[8, 5], [7,4], [9, 6]]]]
 *
 * Bias(1*1*1*1): [[[[1]]]]
 *
 * Filter(1*3*3*2): [[[[1, 5], [7,5], [4, 2]],
 *                  [[1, 8], [3,8], [6, 2]],
 *                  [[4, 8], [5,0], [9, 5]]]]
 * Pad: [1,1,1,1]
 *
 * Stride: [2,2]
 *
 * Dilation: [1,1]
 *
 * Expected Outputs(1*2*2*1): [[[[136], [122]],
 *                            [[195], [176]]]]
 * @version 0.1
 * @date 2023-06-26
 *
 * @copyright Copyright (c) 2023
 *
 */

#include "cnnl.h"
#include "cnrt.h"
#include <bang.h>
#include <fstream>
#include <iostream>
#include <sstream>
#include <stdio.h>
#include <stdlib.h>
#include <string>
#include <time.h>

#define ERROR(s)                                                                                                       \
    {                                                                                                                  \
        std::stringstream _message;                                                                                    \
        _message << __FILE__ << ':' << __LINE__ << " " << std::string(s);                                              \
        std::cerr << _message.str() << "\nAborting...\n";                                                              \
        exit(EXIT_FAILURE);                                                                                            \
    }

std::string cnnlErrorString(cnnlStatus_t status)
{
    switch (status)
    {
    default: {
        return "CNNL_STATUS_UNKNOWN";
    }
    case CNNL_STATUS_SUCCESS: {
        return "CNNL_STATUS_SUCCESS";
    }
    case CNNL_STATUS_NOT_INITIALIZED: {
        return "CNNL_STATUS_NOT_INITIALIZED";
    }
    case CNNL_STATUS_ALLOC_FAILED: {
        return "CNNL_STATUS_ALLOC_FAILED";
    }
    case CNNL_STATUS_BAD_PARAM: {
        return "CNNL_STATUS_BAD_PARAM";
    }
    case CNNL_STATUS_INTERNAL_ERROR: {
        return "CNNL_STATUS_INTERNAL_ERROR";
    }
    case CNNL_STATUS_ARCH_MISMATCH: {
        return "CNNL_STATUS_MISMATCH";
    }
    case CNNL_STATUS_EXECUTION_FAILED: {
        return "CNNL_STATUS_EXECUTION_FAILED";
    }
    case CNNL_STATUS_NOT_SUPPORTED: {
        return "CNNL_STATUS_NOT_SUPPORTED";
    }
    case CNNL_STATUS_NUMERICAL_OVERFLOW: {
        return "CNNL_STATUS_NUMERICAL_OVERFLOW";
    }
    }
}

#define CNNL_CHECK(val)                                                                                                \
    {                                                                                                                  \
        if (val != CNNL_STATUS_SUCCESS)                                                                                \
        {                                                                                                              \
            ERROR(cnnlErrorString(val));                                                                               \
        }                                                                                                              \
    }

struct KernelAttributes
{
    int h = 0;
    int w = 0;
    int stride_h = 1;
    int stride_w = 1;
    int dilation_h = 1;
    int dilation_w = 1;
};

struct DataShape
{
    int n = 0;
    int h = 0;
    int w = 0;
    int c = 0;
    inline int size()
    {
        return n * c * h * w;
    }
};

struct PadAtrributes
{
    int pad_top = 0;
    int pad_bottom = 0;
    int pad_left = 0;
    int pad_right = 0;
};
/**
 * @brief 数据的 shape 参数, 与 conv_sample.cc 不同, 本文将 ShapeParam 与 DataTypeParam 分离
 *
 */
struct ShapeParam
{
    DataShape input;
    DataShape weight; // filter
    DataShape output;
    DataShape bias;
    KernelAttributes kernel;
    PadAtrributes pad;
    bool has_bias = false;
    int group_count = 1;
};

/**
 * @brief 数据的 type 参数
 *
 */
struct DataTypeParam
{
    // input 只支持 int8, int16, int31
    cnnlDataType_t input_dtype;
    // weight 只支持 int8, int16, int31
    cnnlDataType_t weight_dtype;
    // output 只支持 half, float
    cnnlDataType_t output_dtype;
    // !!! 注意 这里数据的格式是 NHWC 不是 NCHW !!!
    cnnlTensorLayout_t layout = CNNL_LAYOUT_NHWC;
};

/**
 * @brief Data address structure
 *
 */
struct DataAddress
{
    // 如果不使用 定点量化技术， 就不需要 cpu_data, fp_data，只需要 host 和 device
    float *cpu_data = nullptr;   // repack quantized data used for cpu compute
    float *fp_data = nullptr;    // store init random data
    void *host_data = nullptr;   // store casted data
    void *device_data = nullptr; // mlu device address
    size_t mlu_size = 0;         // mlu malloc size
    size_t fp_size = 0;          // cpu random data size
};
/**
 * @brief Convolution data structure
 *
 */
struct Convolution
{
    /* 数据描述符 */
    cnnlTensorDescriptor_t input_desc = nullptr;     // 输入数据 描述符
    cnnlTensorDescriptor_t weight_desc = nullptr;    // weight 数据描述符
    cnnlTensorDescriptor_t output_desc = nullptr;    // 输出数据 描述符
    cnnlTensorDescriptor_t bias_desc = nullptr;      // bias 数据描述符
    cnnlConvolutionDescriptor_t conv_desc = nullptr; // convolution 卷积描述符
    cnnlConvolutionForwardAlgo_t algo;               //  卷积算法

    /* 数据地址, 数据大小 */
    DataAddress input_data;
    DataAddress weight_data;
    DataAddress output_data;
    DataAddress bias_data;

    /* 辅助workspace空间 */
    void *workspace = nullptr; // 辅助工作空间 workspace
    size_t workspace_size = 0; // 辅助工作空间 workspace 大小

    double diff1 = 0.0;
    double diff2 = 0.0;
};
/**
 * @brief 初始化 tensor 描述符
 *
 * @param &desc 描述符
 * @param shape data shape
 * @param dtype data type
 * @param layout layout
 */
void setTensorDesc(cnnlTensorDescriptor_t &desc, DataShape &shape, cnnlDataType_t dtype, cnnlTensorLayout_t layout)
{
    int dim[4];
    if (layout == CNNL_LAYOUT_NHWC)
    {
        dim[0] = shape.n;
        dim[1] = shape.h;
        dim[2] = shape.w;
        dim[3] = shape.c;
    }
    else if (layout == CNNL_LAYOUT_NCHW)
    {
        dim[0] = shape.n;
        dim[1] = shape.c;
        dim[2] = shape.h;
        dim[3] = shape.w;
    }
    else if (layout == CNNL_LAYOUT_HWCN)
    {
        dim[0] = shape.h;
        dim[1] = shape.w;
        dim[2] = shape.c;
        dim[3] = shape.n;
    }
    else
    {
        std::cerr << "unsupport data layout!";
    }
    CNNL_CHECK(cnnlCreateTensorDescriptor(&desc));
    CNNL_CHECK(cnnlSetTensorDescriptor(desc, layout, dtype, 4, dim));
}

void initDescriptorAndWorkspace(cnnlHandle_t handle, Convolution &convolution, ShapeParam &shape_param,
                                DataTypeParam &data_type_param)
{
    /* 初始化数据描述符 input, weight, output, bias*/
    setTensorDesc(convolution.input_desc, shape_param.input, data_type_param.input_dtype, data_type_param.layout);
    setTensorDesc(convolution.weight_desc, shape_param.weight, data_type_param.weight_dtype, data_type_param.layout);
    setTensorDesc(convolution.output_desc, shape_param.output, data_type_param.output_dtype, data_type_param.layout);
    // 如果存在 bias 则 初始化 bias 的描述符
    if (shape_param.has_bias)
    {
        setTensorDesc(convolution.bias_desc, shape_param.bias, data_type_param.output_dtype, data_type_param.layout);
    }
    // 卷积描述符的初始化比较复杂
    // 获取 stride, dilation 和 pad 的 attribute
    int pad[4] = {shape_param.pad.pad_top, shape_param.pad.pad_bottom, shape_param.pad.pad_left,
                  shape_param.pad.pad_right}; // top, bottom, left, right
    int stride[2] = {shape_param.kernel.stride_h, shape_param.kernel.stride_w};
    int dilation[2] = {shape_param.kernel.dilation_h, shape_param.kernel.dilation_w};

    CNNL_CHECK(cnnlCreateConvolutionDescriptor(&convolution.conv_desc));
    convolution.algo = CNNL_CONVOLUTION_FWD_ALGO_DIRECT;
    CNNL_CHECK(cnnlSetConvolutionDescriptor(convolution.conv_desc, 4, pad, stride, dilation, shape_param.group_count,
                                            data_type_param.output_dtype));

    // 设置 workspace 辅助空间大小
    CNNL_CHECK(cnnlGetConvolutionForwardWorkspaceSize(
        handle, convolution.input_desc, convolution.weight_desc, convolution.output_desc, convolution.bias_desc,
        convolution.conv_desc, convolution.algo, &(convolution.workspace_size)));

    // 设置 数据内存(host/device端)大小 (byte)
    auto getDataSize = [](cnnlDataType_t dtype) -> size_t {
        switch (dtype)
        {
        case CNNL_DTYPE_HALF:
            return 2;
        case CNNL_DTYPE_FLOAT:
            return 4;
        case CNNL_DTYPE_INT8:
            return 1;
        case CNNL_DTYPE_INT16:
            return 2;
        case CNNL_DTYPE_INT31:
            return 4;
        case CNNL_DTYPE_INT32:
            return 4;
        default:
            std::cout << "unsupport data  dtype.\n";
            return -1;
        }
    };
    convolution.input_data.fp_size = shape_param.input.size() * sizeof(int16_t);
    convolution.input_data.mlu_size = shape_param.input.size() * getDataSize(data_type_param.input_dtype);

    convolution.weight_data.fp_size = shape_param.weight.size() * sizeof(int16_t);
    convolution.weight_data.mlu_size = shape_param.weight.size() * getDataSize(data_type_param.weight_dtype);

    convolution.output_data.fp_size = shape_param.output.size() * sizeof(float);
    convolution.output_data.mlu_size = shape_param.output.size() * getDataSize(data_type_param.output_dtype);
    if (shape_param.has_bias)
    {
        convolution.bias_data.fp_size = shape_param.bias.size() * sizeof(float);
        convolution.bias_data.mlu_size = shape_param.bias.size() * getDataSize(data_type_param.output_dtype);
    }
}
/**
 * @brief 初始化 host 内存空间，数据
 * @param convolution
 * @param shape_param
 * @param input_data
 * @param weight_data
 * @param bias_data
 */
void initHostData(Convolution &convolution, ShapeParam &shape_param, int16_t *input_data, int16_t *weight_data,
                  float *bias_data)
{
    // 申请空间
    convolution.input_data.host_data = (int16_t *)malloc(convolution.input_data.fp_size);
    convolution.weight_data.host_data = (int16_t *)malloc(convolution.weight_data.fp_size);

    if (shape_param.has_bias)
    {
        convolution.bias_data.host_data = (float *)malloc(convolution.bias_data.fp_size);
    }
    // !!! 申请 output_data
    convolution.output_data.host_data = (float *)malloc(convolution.output_data.fp_size);

    // 赋值
    for (int i = 0; i < shape_param.input.size(); i++)
    {
        ((int16_t *)(convolution.input_data.host_data))[i] = input_data[i];
    }

    for (int i = 0; i < shape_param.weight.size(); i++)
    {
        ((int16_t *)(convolution.weight_data.host_data))[i] = weight_data[i];
    }
    if (shape_param.has_bias)
    {
        for (int i = 0; i < shape_param.bias.size(); i++)
        {
            ((float *)(convolution.bias_data.host_data))[i] = bias_data[i];
        }
    }
}
/**
 * @brief 初始化device内存空间
 * @param convolution
 */
void deviceMalloc(Convolution &convolution, ShapeParam &shape_param)
{
    // 初始化 device 内存空间 并且 赋值为 0.0
    CNRT_CHECK(cnrtMalloc((void **)(&convolution.input_data.device_data), convolution.input_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.input_data.device_data, 0, convolution.input_data.mlu_size));

    CNRT_CHECK(cnrtMalloc((void **)(&convolution.weight_data.device_data), convolution.weight_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.weight_data.device_data, 0, convolution.weight_data.mlu_size));

    CNRT_CHECK(cnrtMalloc((void **)(&convolution.output_data.device_data), convolution.output_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.output_data.device_data, 0, convolution.output_data.mlu_size));

    if (shape_param.has_bias)
    {
        CNRT_CHECK(cnrtMalloc((void **)(&convolution.bias_data.device_data), convolution.bias_data.mlu_size));
        CNRT_CHECK(cnrtMemset(convolution.bias_data.device_data, 0, convolution.bias_data.mlu_size));
    }

    std::cout << convolution.input_data.mlu_size << "," << convolution.weight_data.mlu_size << ","
              << convolution.output_data.mlu_size << ";\n";
    if (convolution.workspace_size != 0)
    {
        CNRT_CHECK(cnrtMalloc((void **)(&convolution.workspace), convolution.workspace_size));
        CNRT_CHECK(cnrtMemset(convolution.workspace, 0, convolution.workspace_size));
    }
}
/**
 * @brief 初始化 device 内存空间上的数据
 * @param convolution
 */
void initDeviceData(Convolution &convolution, ShapeParam &shape_param)
{
    // 初始化 device 内存空间 并且 赋值为 0.0
    CNRT_CHECK(cnrtMalloc((void **)(&convolution.input_data.device_data), convolution.input_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.input_data.device_data, 0, convolution.input_data.mlu_size));

    CNRT_CHECK(cnrtMalloc((void **)(&convolution.weight_data.device_data), convolution.weight_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.weight_data.device_data, 0, convolution.weight_data.mlu_size));

    CNRT_CHECK(cnrtMalloc((void **)(&convolution.output_data.device_data), convolution.output_data.mlu_size));
    CNRT_CHECK(cnrtMemset(convolution.output_data.device_data, 0, convolution.output_data.mlu_size));

    if (shape_param.has_bias)
    {
        CNRT_CHECK(cnrtMalloc((void **)(&convolution.bias_data.device_data), convolution.bias_data.mlu_size));
        CNRT_CHECK(cnrtMemset(convolution.bias_data.device_data, 0, convolution.bias_data.mlu_size));
    }

    if (convolution.workspace_size != 0)
    {
        CNRT_CHECK(cnrtMalloc((void **)(&convolution.workspace), convolution.workspace_size));
        CNRT_CHECK(cnrtMemset(convolution.workspace, 0, convolution.workspace_size));
    }

    CNRT_CHECK(cnrtMemcpy(convolution.input_data.device_data, convolution.input_data.host_data,
                          convolution.input_data.mlu_size, CNRT_MEM_TRANS_DIR_HOST2DEV));

    CNRT_CHECK(cnrtMemcpy(convolution.weight_data.device_data, convolution.weight_data.host_data,
                          convolution.weight_data.mlu_size, CNRT_MEM_TRANS_DIR_HOST2DEV));

    if (shape_param.has_bias)
    {
        CNRT_CHECK(cnrtMemcpy(convolution.bias_data.device_data, convolution.bias_data.host_data,
                              convolution.bias_data.mlu_size, CNRT_MEM_TRANS_DIR_HOST2DEV));
    }
}
/**
 * @brief MLU上执行 卷积操作
 * @param handle
 * @param queue
 * @param conv
 */
void convolutionComputeDevice(cnnlHandle_t &handle, cnrtQueue_t &queue, Convolution &conv)
{
    // mlu compute start
    std::cout << "Begin compute.\n";
    // cnrtNotifier_t start = nullptr, end = nullptr;
    // cnrtCreateNotifier(&start);
    // cnrtCreateNotifier(&end);
    // cnrtPlaceNotifier(start, queue);
    // call convolutionForward interface

    CNNL_CHECK(cnnlConvolutionForward(handle, conv.conv_desc, conv.algo, nullptr, conv.input_desc,
                                      conv.input_data.device_data, conv.weight_desc, conv.weight_data.device_data,
                                      conv.bias_desc, conv.bias_data.device_data, conv.workspace, conv.workspace_size,
                                      nullptr, conv.output_desc, conv.output_data.device_data));

    std::cout << "End compute.\n";
    // CNRT_CHECK(cnrtPlaceNotifier(end, queue));
    std::cout << "Begin synchronization.\n";
    CNRT_CHECK(cnrtSyncQueue(queue));
    std::cout << "End synchronization.\n";
    // mlu compute end
    // CNRT_CHECK(cnrtNotifierDuration(start, end, &(conv.hardware_time)));

    // CNRT_CHECK(cnrtDestroyNotifier(&start));
    // CNRT_CHECK(cnrtDestroyNotifier(&end));
}
/**
 * @brief 打印结果
 * @param conv
 */
void printResults(Convolution &conv, cnrtQueue_t &queue, ShapeParam &shape_param)
{

    std::cout << "----- Print Output -----\n";
    std::cout << "Begin copy data from device to host.\n";
    CNRT_CHECK(cnrtMemcpyAsync(conv.output_data.host_data, conv.output_data.device_data, conv.output_data.fp_size,
                               queue, CNRT_MEM_TRANS_DIR_DEV2HOST));
    CNRT_CHECK(cnrtSyncQueue(queue));
    std::cout << "End copy data from device to host.\n";
    std::cout << "Output:[";
    for (int i = 0; i < shape_param.output.size(); i++)
    {
        std::cout << ((float *)conv.output_data.host_data)[i];
        if (i == shape_param.output.size() - 1)
        {
            std::cout << "]\n";
        }
        else
        {
            std::cout << ",";
        }
    }
    std::cout << "-------------------------\n";
}

/**
 * @brief 释放内存
 * @param convolution
 */
void freeDeviceHost(cnnlHandle_t &handle, Convolution &conv, cnrtQueue_t &queue)
{
    // free host space
    free(conv.input_data.host_data);
    free(conv.input_data.fp_data);
    free(conv.input_data.cpu_data);
    free(conv.weight_data.host_data);
    free(conv.weight_data.fp_data);
    free(conv.weight_data.cpu_data);
    free(conv.output_data.host_data);
    free(conv.output_data.fp_data);
    free(conv.output_data.cpu_data);

    if (conv.bias_data.host_data)
    {
        free(conv.bias_data.host_data);
        free(conv.bias_data.fp_data);
    }

    // destory op and tensor desc
    CNNL_CHECK(cnnlDestroyConvolutionDescriptor(conv.conv_desc));
    CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.input_desc));
    CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.weight_desc));
    CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.output_desc));
    if (conv.bias_desc)
    {
        CNNL_CHECK(cnnlDestroyTensorDescriptor(conv.bias_desc));
    }

    // free device memory
    CNRT_CHECK(cnrtFree(conv.input_data.device_data));
    CNRT_CHECK(cnrtFree(conv.weight_data.device_data));
    CNRT_CHECK(cnrtFree(conv.output_data.device_data));
    if (conv.bias_data.device_data != nullptr)
    {
        CNRT_CHECK(cnrtFree(conv.bias_data.device_data));
    }
    if (conv.workspace != nullptr)
    {
        CNRT_CHECK(cnrtFree(conv.workspace));
    }

    // destory queue and runtime context
    CNRT_CHECK(cnrtDestroyQueue(queue));
    CNNL_CHECK(cnnlDestroy(handle));
}

int main(int, char **)
{

    /* !!!!! 数据类型注意事项 !!!!!*/
    /* 根据 cambricon cnnl developer guide, convolutionForward 函数对input, weight, output
    的数据类型有一定的要求(一下只列出 MLU200 series):
    /* input(type id): int8(3), int16(4), int31(5)
    /* weight/filter(type id): int8(3), int16(4), int 31(5)
    /* bias(type id): half(1), float(2)
    /* output(type id): half(1), float(2)
    /* 另外，如果 group_count 等于 input channel,
    /* 那么: input-weight-output 可以等于相同的数据类型 half(1) 或者 float(2)
    /* !!!!! 数据类型注意事项 !!!!!*/

    /* init device 初始化设备 */
    std::cout << "Begin:init device:\n";

    cnrtDev_t dev;                 // 声明 设备变量
    cnrtQueue_t queue = nullptr;   // 声明 队列
    cnnlHandle_t handle = nullptr; // 声明 句柄变量

    unsigned int device_cnt;

    CNRT_CHECK(cnrtInit(0)); // 初始化运行环境, 0 表示初始化实际设备, 1 表示初始化虚拟设备
    CNRT_CHECK(cnrtGetDeviceCount(&device_cnt)); // 获取设备个数 必须在 cnrtInit(0) 之后才能正确获取设备个数
    std::cout << "device count:" << device_cnt << std::endl;
    if (device_cnt <= 0)
    {
        std::cout << "There is no MLU device.\n";
        return -1;
    }

    CNRT_CHECK(cnrtGetDeviceHandle(&dev, 0)); // 获取设备 0 的句柄,存储在 dev 中
    CNRT_CHECK(cnrtSetCurrentDevice(dev)); // 设置当前线程 使用的设备句柄 (据此设置该线程使用的设备)

    CNRT_CHECK(cnrtCreateQueue(&queue)); // 定义一个 队列变量 pQueue
    CNNL_CHECK(cnnlCreate(&handle)); // 创建 cnnl 句柄，此句柄会默认与 cnrtSetCurrentDevice(dev) 中的设备(设备0)绑定
    CNNL_CHECK(cnnlSetQueue(handle,
                            queue)); // 将队列 pQueue 绑定到 handle 中. 用于调用 cnnl 函数时，确定需要用到的 队列 queue
    std::cout << "End:init device:\n";

    // 初始化 handle, 数据(NCHW), 生成描述符, 进行计算, 验证数据
    /* 初始化数据 */
    /* 数据格式 NHWC(1,10,10, 3): 1个 batch, 数据大小为 10*10, 3个通道 */

    std::cout << "Begin:print data shape:\n";
    Convolution convolution;
    /* 初始化数据 shape_param */
    ShapeParam shape_param;
    shape_param.input = {1, 3, 3, 2};
    shape_param.weight = {1, 3, 3, 2};
    shape_param.output = {1, 2, 2, 1};
    shape_param.bias = {1, 1, 1, 1};
    /* 初始化 kernel */
    shape_param.kernel.h = 3;
    shape_param.kernel.w = 3;
    shape_param.kernel.stride_h = 2;
    shape_param.kernel.stride_w = 2;
    shape_param.kernel.dilation_h = 1;
    shape_param.kernel.dilation_w = 1;

    shape_param.pad = {1, 1, 1, 1};
    shape_param.has_bias = false; // 不启用 bias
    shape_param.group_count = 1;  // 只用一个 group
    /* 初始化 data_type_param */
    DataTypeParam data_type_param;
    /* 注意input 和 wwight 的数据类型!!!必须为 int8/int16/int31 其中的一种 */
    data_type_param.input_dtype = cnnlDataType_t::CNNL_DTYPE_INT16;
    data_type_param.weight_dtype = cnnlDataType_t::CNNL_DTYPE_INT16;
    data_type_param.output_dtype = cnnlDataType_t::CNNL_DTYPE_FLOAT;
    // !!! 注意 这里数据的格式是 NHWC 不是 NCHW !!!
    data_type_param.layout = CNNL_LAYOUT_NHWC;

    std::cout << "input shape:n=" << shape_param.input.n << ",c=" << shape_param.input.c << ",h=" << shape_param.input.h
              << ",w=" << shape_param.input.w << ";\n";
    std::cout << "weight shape:n=" << shape_param.weight.n << ",c=" << shape_param.weight.c
              << ",h=" << shape_param.weight.h << ",w=" << shape_param.weight.w << ";\n";
    std::cout << "output shape:n=" << shape_param.output.n << ",c=" << shape_param.output.c
              << ",h=" << shape_param.output.h << ",w=" << shape_param.output.w << ";\n";

    std::cout << "kernel:h=" << shape_param.kernel.h << ",w=" << shape_param.weight.w
              << ",sh=" << shape_param.kernel.stride_h << ",sw=" << shape_param.kernel.stride_w
              << ",dh=" << shape_param.kernel.dilation_h << ",dw=" << shape_param.kernel.dilation_w << ";\n";
    std::cout << "End:print data shape:\n";

    std::cout << "Begin::create input/output tensors descriptors and fp_mlu_data size.\n";
    /* 初始化 数据描述符 和 workspace 空间 */
    initDescriptorAndWorkspace(handle, convolution, shape_param, data_type_param);
    std::cout << "End::create input/output tensors descriptors and fp_mlu_data size.\n";
    // device memory malloc
    /* 初始化数据 value */
    // input,
    // weight,
    // bias,
    int16_t input_data[] = {5, 1, 8, 1, 6, 4, 3, 8, 2, 6, 0, 6, 8, 5, 7, 4, 9, 6};
    int16_t weight_data[] = {1, 5, 7, 5, 4, 2, 1, 8, 3, 8, 6, 2, 4, 8, 5, 0, 9, 5};
    float bias[] = {1.0f};

    std::cout << "Begin::host memory malloc.\n";
    // 先不使用 定点量化 技术
    // 好像必须使用 定点量化技术
    initHostData(convolution, shape_param, input_data, weight_data, bias);
    std::cout << "input:[";
    for (int i = 0; i < shape_param.input.size(); i++)
    {
        std::cout << ((int16_t *)(convolution.input_data.host_data))[i] << ",";
    }
    std::cout << "];\n";

    std::cout << "weight:[";
    for (int i = 0; i < shape_param.weight.size(); i++)
    {
        std::cout << ((int16_t *)(convolution.weight_data.host_data))[i] << ",";
    }
    std::cout << "];\n";

    std::cout << "End::host memory malloc.\n";

    std::cout << "Begin::device memory malloc.\n";
    /* 申请数据空间 */
    // deviceMalloc(convolution, shape_param);
    initDeviceData(convolution, shape_param);
    std::cout << "End::device memory malloc.\n";

    convolutionComputeDevice(handle, queue, convolution);
    printResults(convolution, queue, shape_param);
    // cnrtDestroyQueue(pQueue); // 销毁队列
    // cnrtDestroy();            // 销毁运行环境, 与 cnrtInit() 匹配
    freeDeviceHost(handle, convolution, queue);
    return 0;
}
